{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"SICK.txt\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(df['entailment_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", str(phrase))\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", str(phrase))\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", str(phrase))\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", str(phrase))\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", str(phrase))\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", str(phrase))\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", str(phrase))\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", str(phrase))\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", str(phrase))\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", str(phrase))\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def striphtml(data): \n",
    "    cleanr = re.compile('<.*?>') \n",
    "    cleantext = re.sub(cleanr, ' ', str(data)) \n",
    "    return cleantext  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stripunc(data): \n",
    "    return re.sub('[^A-Za-z]+', ' ', str(data), flags=re.MULTILINE|re.DOTALL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "stemmer = SnowballStemmer(\"english\") \n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(sent): \n",
    "    \n",
    "    sent = decontracted(sent) \n",
    "    sent = striphtml(sent) \n",
    "    sent = stripunc(sent) \n",
    "    \n",
    "    words=word_tokenize(str(sent.lower())) \n",
    "    \n",
    "    #Removing all single letter and and stopwords from question \n",
    "    sent1=' '.join(str(stemmer.stem(j)) for j in words if j not in stop_words and (len(j)!=1)) \n",
    "    sent2=' '.join(str(j) for j in words if j not in stop_words and (len(j)!=1)) \n",
    "    return sent1, sent2   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_stemmed_q1 = []\n",
    "clean_stemmed_q2 = []\n",
    "clean_q1 = []\n",
    "clean_q2 = []\n",
    "combined_stemmed_text = []\n",
    "for _, row in tqdm(df.iterrows()):\n",
    "    csq1, cq1 = compute(row['sentence_A'])\n",
    "    csq2, cq2 = compute(row['sentence_B'])\n",
    "    clean_stemmed_q1.append(csq1)\n",
    "    clean_q1.append(cq1)\n",
    "    clean_stemmed_q2.append(csq2)\n",
    "    clean_q2.append(cq2)\n",
    "    combined_stemmed_text.append(csq1+\" \"+csq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(clean_stemmed_q1))\n",
    "print(len(clean_stemmed_q2))\n",
    "print(len(clean_q1))\n",
    "print(len(clean_q2))\n",
    "print(len(combined_stemmed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_stemmed_q1'] = clean_stemmed_q1\n",
    "df['clean_stemmed_q2'] = clean_stemmed_q2\n",
    "df['clean_q1'] = clean_q1\n",
    "df['clean_q2'] = clean_q2\n",
    "df['combined_stemmed_text'] = combined_stemmed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastext_model = fasttext.load_model(\"crawl-300d-2M-subword.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp, X_test, y_temp, y_test = train_test_split(data[['clean_q1', 'clean_q2']], data['relatedness_score'], test_size=0.35, random_state=21, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.20, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train['text'] = X_train[['clean_q1','clean_q2']].apply(lambda x:str(x[0])+\" \"+str(x[1]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Input, Concatenate, Conv2D, Flatten, Dense, Embedding, LSTM\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(X_train['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['clean_q1'] = X_train['clean_q1'].astype(str)\n",
    "X_train['clean_q2'] = X_train['clean_q2'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val['clean_q1'] = X_val['clean_q1'].astype(str)\n",
    "X_val['clean_q2'] = X_val['clean_q2'].astype(str)\n",
    "\n",
    "X_test['clean_q1'] = X_test['clean_q1'].astype(str)\n",
    "X_test['clean_q2'] = X_test['clean_q2'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q1_seq = t.texts_to_sequences(X_train['clean_q1'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q2_seq = t.texts_to_sequences(X_train['clean_q2'].values)\n",
    "val_q1_seq = t.texts_to_sequences(X_val['clean_q1'].values)\n",
    "val_q2_seq = t.texts_to_sequences(X_val['clean_q2'].values)\n",
    "test_q1_seq = t.texts_to_sequences(X_test['clean_q1'].values)\n",
    "test_q2_seq = t.texts_to_sequences(X_test['clean_q2'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_q1_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_vec = [len(sent_vec) for sent_vec in train_q1_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(len_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_vec = [len(sent_vec) for sent_vec in train_q2_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(len_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q1_seq = pad_sequences(train_q1_seq, maxlen=max_len, padding='pre', value=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q2_seq = pad_sequences(train_q2_seq, maxlen=max_len, padding='pre', value=1.0)\n",
    "val_q1_seq = pad_sequences(val_q1_seq, maxlen=max_len, padding='pre', value=1.0)\n",
    "val_q2_seq = pad_sequences(val_q2_seq, maxlen=max_len, padding='pre', value=1.0)\n",
    "test_q1_seq = pad_sequences(test_q1_seq, maxlen=max_len, padding='pre', value=1.0)\n",
    "test_q2_seq = pad_sequences(test_q2_seq, maxlen=max_len, padding='pre', value=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_q1_seq[0]))\n",
    "print(len(train_q2_seq[0]))\n",
    "print(len(val_q1_seq[0]))\n",
    "print(len(val_q2_seq[0]))\n",
    "print(len(test_q1_seq[0]))\n",
    "print(len(test_q1_seq[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q1_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_present_list = []\n",
    "vocab_size = len(t.word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, len(fastext_model['no'])))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = fastext_model[word]\n",
    "\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        embedding_matrix[i] = np.zeros(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t.word_index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.layers import Input, Dense, Flatten, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, Add, Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponent_neg_manhattan_distance(vects):\n",
    "    ''' Helper function for the similarity estimate of the LSTMs outputs '''\n",
    "    x, y = vects\n",
    "    return K.exp(-K.sum(K.abs(x - y), axis=1, keepdims=True))\n",
    "\n",
    "def exponent_neg_cosine_distance(x, hidden_size=50):\n",
    "    ''' Helper function for the similarity estimate of the LSTMs outputs '''\n",
    "    leftNorm = K.l2_normalize(x[:,:hidden_size], axis=-1)\n",
    "    rightNorm = K.l2_normalize(x[:,hidden_size:], axis=-1)\n",
    "    return K.exp(K.sum(K.prod([leftNorm, rightNorm], axis=0), axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import to_categorical\n",
    "# y_train = pd.get_dummies(y_train)\n",
    "# y_val = pd.get_dummies(y_val)\n",
    "# y_test = pd.get_dummies(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train / 5.0\n",
    "y_val = y_val / 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_network(input_shape):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(name=\"synopsis_embedd\",input_dim =len(t.word_index)+1, \n",
    "                            output_dim=len(fastext_model['no']),weights=[embedding_matrix], \n",
    "                            input_length=train_q1_seq.shape[1],trainable=True))\n",
    "    model.add(LSTM(64,return_sequences=True, activation=\"sigmoid\"))\n",
    "    model.add(LSTM(128,return_sequences=True, activation=\"sigmoid\"))\n",
    "    model.add(LSTM(256,return_sequences=True, activation=\"sigmoid\"))\n",
    "    model.add(LSTM(256,return_sequences=True, activation=\"sigmoid\"))\n",
    "    model.add(LSTM(128,return_sequences=True, activation=\"sigmoid\"))\n",
    "    model.add(LSTM(64,return_sequences=False, activation=\"sigmoid\"))\n",
    "    model.add(Dense(128, activation='sigmoid'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = train_q1_seq.shape[1]\n",
    "sent_a = Input(shape=(input_dim,))\n",
    "sent_b = Input(shape=(input_dim,))\n",
    "print('input_dim',input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_network = build_base_network(input_dim)\n",
    "feat_vecs_a = base_network(sent_a)\n",
    "feat_vecs_b = base_network(sent_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_vecs_b.get_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = Lambda(exponent_neg_manhattan_distance, output_shape=(1,))([feat_vecs_a, feat_vecs_b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concats = concatenate([feat_vecs_a, feat_vecs_b], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = Dense(1, activation='softmax')(concats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "epochs = 20\n",
    "rms = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)#RMSprop()\n",
    "adadelta = optimizers.Adadelta(lr=0.001, rho=0.95, epsilon=1e-07)\n",
    "adagrad = optimizers.Adagrad(lr=0.0001, epsilon=1e-02)\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=5,\n",
    "                              verbose=1,\n",
    "                              restore_best_weights=True)\n",
    "callback_early_stop_reduceLROnPlateau=[earlyStopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    margin = 1\n",
    "    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))\n",
    "\n",
    "def compute_accuracy(predictions, labels):\n",
    "    '''Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    return labels[predictions.ravel() < 0.5].mean()\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    '''Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input=[sent_a, sent_b], output=distance)\n",
    "model.compile(loss='mean_absolute_error', optimizer=rms, metrics=['mse'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit([train_q1_seq,train_q2_seq], y_train, validation_data=([val_q1_seq, val_q2_seq], y_val),\n",
    "                        epochs=20, batch_size=32, verbose=1, shuffle = True, callbacks=callback_early_stop_reduceLROnPlateau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Accuracy\"\n",
    "plt.figure(1)\n",
    "plt.plot(history.history['mean_squared_error'])\n",
    "plt.plot(history.history['val_mean_squared_error'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# \"Loss\"\n",
    "plt.figure(2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate([train_q1_seq,train_q2_seq], y_train, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test = model.evaluate([val_q1_seq,val_q2_seq], y_val, verbose=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
